\section{Implementation and Evaluation}
\label{section:evaluation}

We have implemented our string constraint solving procedure in a tool called {\tool}. {\tool} is implemented as a theory solver of the SMT solver Z3~\cite{z3}. In this way, we can concentrate on solving conjunctive constraints and let Z3 handle the other boolean connectives. Secondly, it makes it possible to solve not only formulae over string constraints but also combinations of string constraints with other theories that Z3 supports. Furthermore, this approach allows us to more effectively handle the arithmetic constraints that are generated by the under-approximation module and, lastly, it eliminates the need to have our own parser. 

In {\tool}, we use the following PFA selection strategy. We use \emph{numeric PFAs} for string variables appearing in string-number conversion and \emph{standard PFAs} for others. We select a size $m$ for numeric PFAs, a number $p$ of their loops, and the length $q$ of the loops. Initially, we set $(m,p,q)=(5,2,q)$ where $q$ is dynamic and obtained from our internal static analysis. We double $m$ and increase $p$ and $q$ by one if refinement is required. We set an upper bound for each parameter and report UNKNOWN if a solution cannot be found within the bound.

Our over-approximation module also uses heuristics to derive the constant value of any side of the constraint $n=\sti{x}$ to refine the over-approximation. For instance, assume we can derive that $n=12$ from some integer constraints. Then we can derive the value of $x$ belongs to the regular language $(0^*12)$. 
%HERE

%more or less standard
The way our theory solver and Z3 interact is almost standard. When Z3 asks our theory solver a string constraint satisfiability problem, our solver tries to prove it is SAT or UNSAT using the procedure discussed in this paper. For under-approximation, whenever a corresponding linear formula is created, we attach the current value of $m$, $p$, $q$ to the formula, and then push it to Z3 core. If our theory solver reports UNKNOWN, Z3 remembers it in a global flag \textsf{incomplete} and either tries another solution branch, or the same solution branch with different value of $m$, $p$, $q$. If Z3 completes the search of all solution branches, it reports UNSAT if the flag \textsf{incomplete} is down, and UNKNOWN otherwise.


We compare {\tool} (\texttt{1e715b7dab})\footnote{\url{https://github.com/guluchen/z3/tree/1e715b7dab}} with other state-of-the-art string solvers, namely, CVC4~(version 1.7)~\cite{cvc4Tool}, Z3~(version 4.8.7)~\cite{z3}, and \textsf{Z3Str3}~(version 4.8.7)~\cite{zheng2013z3}. For these tools, the versions we used are the latest release version. \changed{Observe that CVC4 and Z3 are  DPLL(T)-based string solvers.}
We do not compare with Sloth \cite{sloth} since it does not support length constraints, which occur in most of our benchmarks. We also do not compare with ABC~\cite{aydin2018parameterized} (a model counter for string constraints), Ostrich~\cite{chen2019decision} and \textsf{Trau+}~\cite{abdulla2019chain}, because they do not support many of the string functions in our benchmarks, especially string-number conversion.

We perform two sets of experiments. In the first set of experiments, we compare {\tool} with other tools on existing benchmarks over basic string constraints. Those benchmarks do not involve string-number conversion functions. In the second set of experiments, we compare {\tool} with the other tools on new suites focusing on string-number conversion. Our goals of experiments are the following:
\smallskip


\begin{itemize}
	\item {\tool} performs as good as or better than the other tools in solving the  satisfiability problems of basic string constraints.
	
		\smallskip

	\item {\tool} performs significantly better than the other tools in solving the satisfiability problems on string-number conversion benchmark, and this shows  the efficiency of PFA in general and \emph{numeric} PFA in particular.
\end{itemize}
		\smallskip

In the first set of experiments, we use the following benchmark examples:

		\smallskip


\begin{itemize}
	\item PyEx~\cite{pyex} comes from running the symbolic executor PyEx over some Python packages.
	
	\smallskip
	


	\item \changed{ LeetCode comes from running PyEx over a sample code collected from the LeetCode~\cite{LeetCode} website, including functions that check whether a string is a valid IPv4 or IPv6 address, sum up two binary numbers, check whether an input string is an abbreviation of another input string, and convert a sequence of digits to a string according to a given mapping.}
	
		\smallskip

	\item StringFuzz~\cite{blotsky2018stringfuzz}  is generated by the fuzz testing tool of the same name.
	
		\smallskip

	\item $\text{cvc4}_{\text{pred}}$ and $\text{cvc4}_{\text{term}}$ are obtained from the CVC4 group~\cite{termEQ}. These benchmarks contain a small amount of string-number conversion constraints (< $5\%$).
\end{itemize}

In the second experiment, we compare with tools supporting string-number conversion on the benchmarks collected from the symbolic executor \texttt{Py-Conbyte}\footnote{\url{https://github.com/spencerwuwu/py-conbyte}}, which has the supports of string-number conversion. We ran it on several examples collected from the LeetCode platform and from Python core libraries, which involve diverse usages of string-number conversion in Python such as parsing date-time, verifying and restoring IP addresses from strings, etc. We also have examples that encode execution paths of some JavaScript programs (the Luhn algorithm and some array manipulations).

	\changed{All experiments were executed on a machine with 4-core CPU and 16 GiB RAM.} The timeout was set to 10s for each test.
We use the results from {\tool}, CVC4, and Z3 as the reference answer for the validation of the correctness of the results. Occasionally, two of them report inconsistent  answers (one SAT and one UNSAT). To decide which solver is right, we developed a validator. It takes the model $I$ returned from the solver who reported SAT, assigns $I(x)$ to all variables $x$ in the test to obtain a modified test, and re-evaluates the modified test by multiple solvers. If the results from all solvers are consistent, we mark the test SAT or UNSAT according to the results. Otherwise, we manually simplify and inspect the test until we get a conclusive result. 

\changed{The results of the experiments are summarized in Table~\ref{table:base_benchmark}, Table~\ref{table:str_int_benchmark}, and Table~\ref{table:checkLuhn}. Rows with heading \texttt{SAT}/\texttt{UNSAT} show numbers of solved formulae. Rows with heading \texttt{UNKNOWN} or \texttt{TIMEOUT} indicate the number of instances for which the solver fails to return an answer. \texttt{ERROR} means system crashes due to various reasons (usually out of memory). \texttt{INCORRECT} shows the number of cases where the tool gives a wrong answer.}


\begin{table}[h]
\changed{
\centering
\caption{Results of {\tool}, CVC4, Z3, and Z3Str3 on Basic String Constraint benchmarks.}
\scalebox{0.85}{
\begin{tabular}{|l r | r r r r |}
\hline
\multicolumn{2}{|c}{}                  & {\tool} & CVC4  &Z3 & Z3Str3 \\ \hline
\multirow{6}{*}{PyEx}		& SAT      & 21377& 20350& 18492&  3037 \\
							& UNSAT    &  3860&  3841&  3847&  3816 \\
							& UNKNOWN  &     0&     0&     0&     7 \\
							& TIMEOUT  &   184&  1230&  3082& 16872 \\
							& ERROR    &     0&     0&     0&  1675 \\
							& INCORRECT&     0&     0&     0&    14 \\ \hline
\multirow{6}{*}{LeetCode}	& SAT      &   877&   874&   881&   661 \\
							& UNSAT    &  1785&  1785&  1785&  1780 \\
							& UNKNOWN  &     0&     0&     0&   122 \\
							& TIMEOUT  &    0&    7&     0&    90 \\
							& ERROR    &     4&     0&     0&    13 \\
							& INCORRECT&     0&     0&     0&     0 \\ \hline
\multirow{6}{*}{StringFuzz}	& SAT      &   515&   615&   338&   505 \\
							& UNSAT    &   301&   255&   198&   192 \\
							& UNKNOWN  &     0&     0&     0&     4 \\
							& TIMEOUT  &   249&   195&   529&   364 \\
							& ERROR    &     0&     0&     0&     0 \\
							& INCORRECT&     0&     0&     0&     0 \\\hline
\multirow{6}{*}{cvc4\textsubscript{pred}} & SAT &    13&    11&    12&     8 \\
							& UNSAT    &   822&   818&   808&   774 \\
							& UNKNOWN  &     0&     0&     0&     4 \\
							& TIMEOUT  &     0&     6&    15&    38 \\
							& ERROR    &     0&     0&     0&     11 \\
							& INCORRECT&     0&     0&     0&     0 \\ \hline
\multirow{6}{*}{cvc4\textsubscript{term}} & SAT &    10&     9&     7&    2 \\
							& UNSAT    &  1032&  1026&  1022&   957 \\
							& UNKNOWN  &     0&     0&     0&     3 \\
							& TIMEOUT  &     3&    10&    16&    58 \\
							& ERROR    &     0&     0&     0&     11 \\
							& INCORRECT&     0&     0&     0&    14 \\ \hline \hline
\multirow{6}{*}{Total} 		& SAT      & 22792& 21859& 19730&  4213\\
							& UNSAT    &  7800&  7725&  7550&  7519\\
							& UNKNOWN  &     0&     0&     0&   140\\
							& TIMEOUT  &  436&  1448&  3642& 17422 \\
							& ERROR    &     4&     0&     0&  1710\\
							& INCORRECT&     0&     0&     0&    28 \\\hline
\end{tabular}}
\label{table:base_benchmark}}
\end{table}

\begin{table}[h]
\changed{
\centering
\caption{Results of {\tool}, CVC4, Z3, and Z3Str3 on String-Number Conversion benchmark.}
\scalebox{0.8}{
\begin{tabular}{|l r | r r r r|}
\hline
\multicolumn{2}{|c}{}          			   & {\tool} &  CVC4 &    Z3 & Z3Str3 \\ \hline
\multirow{6}{*}{Leetcode}		& SAT      &  2501&  1659 &  1993&   239 \\ 
								& UNSAT    & 16394& 15604& 16124& 15288 \\
								& UNKNOWN  &     0&     0&     0&   623 \\
								& TIMEOUT  &    32&  1664&   810&  2337 \\
								& ERROR    &     0&     0&     0&   332 \\
								& INCORRECT&     0&     0&     0&   108 \\ \hline 
\multirow{6}{*}{PythonLib}		& SAT      &  1922&   560&   1839&   206 \\ 
								& UNSAT    &   724&   666&   724&   642 \\
								& UNKNOWN  &     0&     0&     0&    45 \\
								& TIMEOUT  &     0&  1420&  83&  1710 \\
								& ERROR    &     0&     0&     0&    41 \\
								& INCORRECT&     0&     0&     0&     2 \\ \hline
\multirow{6}{*}{JavaScript}		& SAT      &    20&     3&    16&     4 \\ 
								& UNSAT                &     0&     0&     0&     0 \\
								& UNKNOWN         &     0&     9&     0&     0 \\
								& TIMEOUT            &     0&     8&     4&    10 \\
								& ERROR                &     0&     0&     0&     6 \\
								& INCORRECT&     0&     0&     0&     0 \\ \hline \hline
\multirow{6}{*}{Total}			& SAT      &  4443&  2222     &  3848    &   449 \\ 
								& UNSAT            & 17118 & 16270     & 16848   & 15930 \\
								& UNKNOWN      &     0   &     9        &     0       &   668 \\
								& TIMEOUT         &    32  &  3092     &  897     &  4057 \\
								& ERROR             &     0   &     0        &     0        &   379 \\
								& INCORRECT     &     0   &     0        &     0         &   110 \\ \hline
\end{tabular}}
\label{table:str_int_benchmark}
}
\end{table}




From Table~\ref{table:base_benchmark}, we can see that the performance of {\tool} is as good as that of the most competitive tools such as CVC4 and Z3 on basic string constraints. In all of the benchmarks, {\tool} ranked either the 1st or the 2nd on the number of solved (SAT+UNSAT) cases. On the StringFuzz benchmarks that are SAT, {\tool} does not perform as well as the best performing tool. We however do not consider this crucial because these benchmarks are just randomly generated for debugging. On the most important benchmarks, those that come from program analysis, {\tool} is comparable to the best performing tool.


\changed{
From Table~\ref{table:str_int_benchmark}, we can see that {\tool} significantly outperforms all the other tools. 
The second best tool, Z3, fails on 50 times more examples. 
}

As an addition experiment, we have encoded the checkLuhn algorithm introduced in Section~\ref{section:introduction} for the cases with 2 to 12 loops (digits). We ran these tests 
%additionally to the experiments above 
with the timeout set to 120s. The result is summarized in Table~\ref{table:checkLuhn}.
In these tests, {\tool} can solve all problems within 1s while CVC4 only returns a model for cases of 2 to 5 loops and Z3Str3 could not solve any of these problems (either TIMEOUT, ERROR, or UNKNOWN). However, Z3 can still solve 5 out of the 11 problems. The behavior of Z3 is not entirely unexpected. All the problems are satisfiable and the solver may be lucky to guess the solution quickly. % branch with a correct model quickly.

\begin{table}[h]
\changed{

\centering
\caption{Comparison of {\tool}, CVC4, Z3, and Z3Str3 with checkLuhn problems of 2 to 12 loops.}
\scalebox{0.8}{
\begin{tabular}{| c | c c c c|}
\hline
\# of Loops & {\tool} 			   &  CVC4    	 		  &       Z3   			& Z3Str3 \\ \hline
2 			& \textbf{SAT}(0.27s)  &  \textbf{SAT}(0.17s) & \textbf{SAT}(0.11s) & \textbf{ERROR} \\
3  			& \textbf{SAT}(0.29s)  &  \textbf{SAT}(6.94s) & \textbf{SAT}(0.13s) & \textbf{ERROR} \\
4 			& \textbf{SAT}(0.37s)  &  \textbf{SAT}(4.92s) &  \textbf{SAT}(0.24s)     & \textbf{ERROR} \\
5 			& \textbf{SAT}(0.39s)  &  \textbf{SAT}(30.86s)&  \textbf{SAT}(0.13s)     & \textbf{ERROR} \\
6 			& \textbf{SAT}(0.41s)  &  \textbf{TIMEOUT}    &\textbf{TIMEOUT} & \textbf{UNKNOWN} \\
7 			& \textbf{SAT}(0.51s)  &  \textbf{TIMEOUT}    & \textbf{TIMEOUT}    & \textbf{ERROR} \\
8 			& \textbf{SAT}(0.53s)  &  \textbf{TIMEOUT}    & \textbf{TIMEOUT} & \textbf{ERROR} \\
9 			& \textbf{SAT}(0.63s)  &  \textbf{TIMEOUT}    &\textbf{SAT}(0.31s)  & \textbf{ERROR} \\
10 			& \textbf{SAT}(0.69s)  &  \textbf{TIMEOUT}    &  \textbf{TIMEOUT} & \textbf{TIMEOUT} \\
11 			& \textbf{SAT}(0.71s)  &  \textbf{TIMEOUT}    &  \textbf{TIMEOUT} & \textbf{ERROR} \\
12 			& \textbf{SAT}(0.74s)  &  \textbf{TIMEOUT}    &  \textbf{TIMEOUT} & \textbf{ERROR} \\ \hline
\end{tabular}}
\label{table:checkLuhn}}
\end{table}


